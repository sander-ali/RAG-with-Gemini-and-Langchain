{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7/FaW++8AXH5v02OEyVZB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sander-ali/RAG-with-Gemini-and-Langchain/blob/main/RAG_with_Gemini_and_Langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](https://github.com/sander-ali/images_to_use/blob/main/logo.jpg?raw=true)"
      ],
      "metadata": {
        "id": "9Wz3q7S7IV5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Install necessary libraries\n",
        "!pip install -q --upgrade google-generativeai langchain-google-genai chromadb pypdf\n"
      ],
      "metadata": {
        "id": "uf-zqZiaIW5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0pSzQsqH4vW"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "import textwrap\n",
        "\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('â€¢', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "39ND6rpGJqpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get your API Key from google studio and store it in your secrets\n",
        "import os\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "irWkxo7HJsr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Generation"
      ],
      "metadata": {
        "id": "EPf94PlNJyoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(model_name = \"gemini-pro\")\n",
        "model"
      ],
      "metadata": {
        "id": "A9_ijrbxJz63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\"What are the usecases of LLMs?\")"
      ],
      "metadata": {
        "id": "Gkqon5XRJ7go"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_markdown(response.text)"
      ],
      "metadata": {
        "id": "kPmE9ljEMrTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use LangChain to Access Gemini API"
      ],
      "metadata": {
        "id": "SE341MEQMuZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI"
      ],
      "metadata": {
        "id": "BYlBpJzxMwM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\",google_api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "OI3dgmCMM0JX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = llm.invoke(\"What are the usecases of LLMs?\")"
      ],
      "metadata": {
        "id": "dxCzQ1JvM2RY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_markdown(result.content)"
      ],
      "metadata": {
        "id": "lhQYNJ3wM4Do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gemini Pro Vision"
      ],
      "metadata": {
        "id": "uc9dWhjSNy9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL.Image\n",
        "\n",
        "img = PIL.Image.open('sample_img.jpg')\n",
        "img"
      ],
      "metadata": {
        "id": "DNYV88kHN2yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](https://github.com/sander-ali/images_to_use/blob/main/sample_img.jpg?raw=true)"
      ],
      "metadata": {
        "id": "ZwnqjpDuPtPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro-vision\",google_api_key=GOOGLE_API_KEY)\n",
        "# example\n",
        "message = HumanMessage(\n",
        "    content=[\n",
        "        {\n",
        "            \"type\": \"text\",\n",
        "            \"text\": \"Write a short description about the product shown in the image for mentioning it on ecommerce website\",\n",
        "        },\n",
        "        {\"type\": \"image_url\", \"image_url\": \"/content/sample_img.jpg\"}, #Alternatively you can paste the url of the image as well\n",
        "    ]\n",
        ")\n",
        "llm.invoke([message])"
      ],
      "metadata": {
        "id": "2WrFBQ8jPw9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat with Documents using RAG (Retreival Augment Generation)"
      ],
      "metadata": {
        "id": "qdcqt8T5P-b-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL.Image\n",
        "\n",
        "img = PIL.Image.open('/content/langchain.jpg')\n",
        "img"
      ],
      "metadata": {
        "id": "bY52gQuwP__0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](https://github.com/sander-ali/images_to_use/blob/main/langchain.jpg?raw=true)"
      ],
      "metadata": {
        "id": "K74sRrd9Tn7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Install dependencies\n",
        "!sudo apt -y -qq install tesseract-ocr libtesseract-dev\n",
        "\n",
        "!sudo apt-get -y -qq install poppler-utils libxml2-dev libxslt1-dev antiword unrtf poppler-utils pstotext tesseract-ocr flac ffmpeg lame libmad0 libsox-fmt-mp3 sox libjpeg-dev swig\n",
        "\n",
        "!pip install langchain"
      ],
      "metadata": {
        "id": "rPsu1NQKTqX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import necessary packages\n",
        "import urllib\n",
        "import warnings\n",
        "from pathlib import Path as p\n",
        "from pprint import pprint\n",
        "\n",
        "import pandas as pd\n",
        "from langchain import PromptTemplate\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# restart python kernal if issues with langchain import."
      ],
      "metadata": {
        "id": "D2k0QnRoT0kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI"
      ],
      "metadata": {
        "id": "MbCCXooAT4An"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatGoogleGenerativeAI(model=\"gemini-pro\",google_api_key=GOOGLE_API_KEY,\n",
        "                             temperature=0.2,convert_system_message_to_human=True)"
      ],
      "metadata": {
        "id": "4Y1kjsvsT6tf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract text from the PDF"
      ],
      "metadata": {
        "id": "Fkyl2vK4T7nf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I am using a very recent paper titled \"MemoRAG: Moving towards Next-Gen RAG Via Memory-Inspired Knowledge Discovery\"\n",
        "pdf_loader = PyPDFLoader(\"/content/2409.05591v2.pdf\")\n",
        "pages = pdf_loader.load_and_split()\n",
        "print(pages[2].page_content)"
      ],
      "metadata": {
        "id": "Ad6qipQWT9u3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(pages)"
      ],
      "metadata": {
        "id": "f8VHN4-UVKLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG Pipeline: Embedding + Gemini (LLM)"
      ],
      "metadata": {
        "id": "cA9F4eY_VL2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings"
      ],
      "metadata": {
        "id": "bjD8lMC_VM-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)\n",
        "context = \"\\n\\n\".join(str(p.page_content) for p in pages)\n",
        "texts = text_splitter.split_text(context)"
      ],
      "metadata": {
        "id": "bckSPZrbVQ8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",google_api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "_UKHfQMKVSxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_index = Chroma.from_texts(texts, embeddings).as_retriever(search_kwargs={\"k\":5})"
      ],
      "metadata": {
        "id": "y3WjxaR5VVF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    model,\n",
        "    retriever=vector_index,\n",
        "    return_source_documents=True\n",
        ")"
      ],
      "metadata": {
        "id": "XJ6mOIx1VXPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is the difference between Standard RAG and MemoRAG?\"\n",
        "result = qa_chain({\"query\": question})\n",
        "result[\"result\"]"
      ],
      "metadata": {
        "id": "FO5R0GvbVZnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(result[\"result\"])"
      ],
      "metadata": {
        "id": "bw8r4FDEVnH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result[\"source_documents\"]"
      ],
      "metadata": {
        "id": "_nRTHUH3Vo-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer.\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)# Run chain\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    model,\n",
        "    retriever=vector_index,\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
        ")"
      ],
      "metadata": {
        "id": "U-MGGkeZVyev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is the difference between Standard RAG and MemoRAG?\"\n",
        "result = qa_chain({\"query\": question})\n",
        "result[\"result\"]"
      ],
      "metadata": {
        "id": "wv4CQunXV3XH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(result[\"result\"])"
      ],
      "metadata": {
        "id": "Xbur3ZhWV5KG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Describe Quasar and Black Hole?\"\n",
        "result = qa_chain({\"query\": question})\n",
        "Markdown(result[\"result\"])"
      ],
      "metadata": {
        "id": "2saxeMGVV8dX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fl6RIYmKVyNx"
      }
    }
  ]
}